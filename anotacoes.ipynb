{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c640e22d-1c6a-47f4-858a-145ff5d4a5d2",
   "metadata": {},
   "source": [
    "Notebook de base para armazenar as minhas anotações dos cursos.\n",
    "\n",
    "Tomarei como base os microdados do enem de 2019. Disponibilizados no [link](https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/enem) . Consultado em 07/01/2021\n",
    "\n",
    "Por Motivos de Volume de dados e possivel problemas de privacidade eu adicionei a pasta de dados, onde eu vou deixar os dados que vou utilizar nos estudos, no .gitignore desse repositorio. Caso seja do interesse reproduzir o conteudo do que esta ocorrendo aqui, recomendo criar a pasta da mesma forma. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff728bdf-578b-42d1-99eb-ca67b4854f96",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "- A instalação do pyspark varia, dependendo do sistema operacional. Como sempre, recomendo que seja utilizado o linux para a instalação. uma VM já resolve. No meu caso estou rodando o Archlinux\n",
    "\n",
    "- Para a instalação do Spark, recomendo que seja seguido o tutorial que foi disponibilizado pelo pessoal da sundog Education, para [linux](https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm), para windows e mac podem ser acompanhado conforme consta no [site](https://sundog-education.com/spark-python/) deles.\n",
    "\n",
    "- Importante lembrar que, o uso do spark não é recomendado para situações de maquina local, uma vez que, caso seja viavel rodar em um nó unico podemos perfeitamente utilizar o pandas. Eu particularmente gosto de usar o pyspark em uma configuração local para testes menores e preparo para uso em um cluster de produção ou infraestrutura similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760916d-bcbb-4561-8328-e4c6dee45748",
   "metadata": {},
   "source": [
    "### importando o pySpark\n",
    "\n",
    "- Após a instalação, para importar a biblioteca base do pyspark funciona de forma muito simples, com o comando import\n",
    "\n",
    "``` import pyspark ``` \n",
    "\n",
    "- Eu não recomendo que seja utilizado o findmyspark, já que em um ambiente de produção, um AWS EMR por exemplo, não existe a necessidade de ser importado dessa forma, o que pode levar a retrabalhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722d94ad-6f79-4604-b1e8-74412fe5eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc55b4e-0669-41b9-b2b3-45bd131018aa",
   "metadata": {},
   "source": [
    "### Criando a sessão\n",
    "- Feito a importação, para executarmos qualquer coisa no spark, precisamos que seja criada uma sessão.\n",
    "    - A sessão no spark pertence a biblioteca SQL, então para criar precissamos chamar a biblioteca SQL e então o SparkSession.\n",
    "    - Do SparkSession, precisamos do metodo builder, que funciona para montar a sessão.\n",
    "    - Após, precisamos do metodo getOrCreate que cria a sessão de fato.\n",
    "    - A Sessão que criarmos podemos armazenar em uma variavel do python, para podemos executar mais etapas do nosso código.\n",
    "    - Assim o comando para criarmos a sessão fica:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21328fbc-cb33-4266-b8dc-86c006b58eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessao = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8780a-cae8-45a0-aee7-859a80043bb5",
   "metadata": {},
   "source": [
    "- Com a Sessão criada, agora podemos executar comandos no spark.\n",
    "\n",
    "### Importando dados\n",
    "- Como vamos usar o spark para processos de ETL, vamos precisar fazer a importação de dados. O spark de forma nativa é capaz de ler os principais tipos de dados que são capazes de se comportar bem com volume altos de dados, então CSV, Parquet, avro, json. O uso do jdbc é bem comum para consulta em bancos de dados relacionais. \n",
    "- Para importar um csv que esta armazenado de forma local, podemos usar o metodo ```read```  na sessão do spark que criamos previamente. Após o metodo read precisamos informar qual tipo de dado que vamos ler, nesse caso vamos ler um arquivo csv, então usamos a função ```csv``` e apontamos o caminho do arquivo que vamos ler, podendo ele estar dentro de um hdfs, um bucket ou local como é o nosso caso.\n",
    "- Quando fomos utilizar o csv como arquivo de base, é recomendavel que seja definido o separador que foi utilizado para gerar o arquivo, no nosso caso foi utilizado o ;, entao passamos o parametro ```sep=';'``` junto com o caminho do arquivo\n",
    "- Outro ponto que é relevante mencionar no csv, é o header, que por padrão o spark não considera a primeira linha como header, caso seja esse o caso do arquivo que estamos lendo, podemos utilizar o parametro ```header``` definindo ele como true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df7076a-85bb-4e27-8bcb-b36ee5251839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemos o csv sem utilizar o separador e nem o header\n",
    "csv_errado = sessao.read.csv('dados/MICRODADOS_ENEM_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecbeea42-1013-4e62-9e7f-a39797993f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_errado.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cf9eab-3a9e-4db9-9467-ef361eede90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemos o mesmo arquivo, agora utilizando o separador e definindo a primeira linha como header\n",
    "csv = sessao.read.csv('dados/MICRODADOS_ENEM_2019.csv', sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ecad4a-0c64-4b27-bf88-de7ea435e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NU_INSCRICAO: string (nullable = true)\n",
      " |-- NU_ANO: string (nullable = true)\n",
      " |-- CO_MUNICIPIO_RESIDENCIA: string (nullable = true)\n",
      " |-- NO_MUNICIPIO_RESIDENCIA: string (nullable = true)\n",
      " |-- CO_UF_RESIDENCIA: string (nullable = true)\n",
      " |-- SG_UF_RESIDENCIA: string (nullable = true)\n",
      " |-- NU_IDADE: string (nullable = true)\n",
      " |-- TP_SEXO: string (nullable = true)\n",
      " |-- TP_ESTADO_CIVIL: string (nullable = true)\n",
      " |-- TP_COR_RACA: string (nullable = true)\n",
      " |-- TP_NACIONALIDADE: string (nullable = true)\n",
      " |-- CO_MUNICIPIO_NASCIMENTO: string (nullable = true)\n",
      " |-- NO_MUNICIPIO_NASCIMENTO: string (nullable = true)\n",
      " |-- CO_UF_NASCIMENTO: string (nullable = true)\n",
      " |-- SG_UF_NASCIMENTO: string (nullable = true)\n",
      " |-- TP_ST_CONCLUSAO: string (nullable = true)\n",
      " |-- TP_ANO_CONCLUIU: string (nullable = true)\n",
      " |-- TP_ESCOLA: string (nullable = true)\n",
      " |-- TP_ENSINO: string (nullable = true)\n",
      " |-- IN_TREINEIRO: string (nullable = true)\n",
      " |-- CO_ESCOLA: string (nullable = true)\n",
      " |-- CO_MUNICIPIO_ESC: string (nullable = true)\n",
      " |-- NO_MUNICIPIO_ESC: string (nullable = true)\n",
      " |-- CO_UF_ESC: string (nullable = true)\n",
      " |-- SG_UF_ESC: string (nullable = true)\n",
      " |-- TP_DEPENDENCIA_ADM_ESC: string (nullable = true)\n",
      " |-- TP_LOCALIZACAO_ESC: string (nullable = true)\n",
      " |-- TP_SIT_FUNC_ESC: string (nullable = true)\n",
      " |-- IN_BAIXA_VISAO: string (nullable = true)\n",
      " |-- IN_CEGUEIRA: string (nullable = true)\n",
      " |-- IN_SURDEZ: string (nullable = true)\n",
      " |-- IN_DEFICIENCIA_AUDITIVA: string (nullable = true)\n",
      " |-- IN_SURDO_CEGUEIRA: string (nullable = true)\n",
      " |-- IN_DEFICIENCIA_FISICA: string (nullable = true)\n",
      " |-- IN_DEFICIENCIA_MENTAL: string (nullable = true)\n",
      " |-- IN_DEFICIT_ATENCAO: string (nullable = true)\n",
      " |-- IN_DISLEXIA: string (nullable = true)\n",
      " |-- IN_DISCALCULIA: string (nullable = true)\n",
      " |-- IN_AUTISMO: string (nullable = true)\n",
      " |-- IN_VISAO_MONOCULAR: string (nullable = true)\n",
      " |-- IN_OUTRA_DEF: string (nullable = true)\n",
      " |-- IN_GESTANTE: string (nullable = true)\n",
      " |-- IN_LACTANTE: string (nullable = true)\n",
      " |-- IN_IDOSO: string (nullable = true)\n",
      " |-- IN_ESTUDA_CLASSE_HOSPITALAR: string (nullable = true)\n",
      " |-- IN_SEM_RECURSO: string (nullable = true)\n",
      " |-- IN_BRAILLE: string (nullable = true)\n",
      " |-- IN_AMPLIADA_24: string (nullable = true)\n",
      " |-- IN_AMPLIADA_18: string (nullable = true)\n",
      " |-- IN_LEDOR: string (nullable = true)\n",
      " |-- IN_ACESSO: string (nullable = true)\n",
      " |-- IN_TRANSCRICAO: string (nullable = true)\n",
      " |-- IN_LIBRAS: string (nullable = true)\n",
      " |-- IN_TEMPO_ADICIONAL: string (nullable = true)\n",
      " |-- IN_LEITURA_LABIAL: string (nullable = true)\n",
      " |-- IN_MESA_CADEIRA_RODAS: string (nullable = true)\n",
      " |-- IN_MESA_CADEIRA_SEPARADA: string (nullable = true)\n",
      " |-- IN_APOIO_PERNA: string (nullable = true)\n",
      " |-- IN_GUIA_INTERPRETE: string (nullable = true)\n",
      " |-- IN_COMPUTADOR: string (nullable = true)\n",
      " |-- IN_CADEIRA_ESPECIAL: string (nullable = true)\n",
      " |-- IN_CADEIRA_CANHOTO: string (nullable = true)\n",
      " |-- IN_CADEIRA_ACOLCHOADA: string (nullable = true)\n",
      " |-- IN_PROVA_DEITADO: string (nullable = true)\n",
      " |-- IN_MOBILIARIO_OBESO: string (nullable = true)\n",
      " |-- IN_LAMINA_OVERLAY: string (nullable = true)\n",
      " |-- IN_PROTETOR_AURICULAR: string (nullable = true)\n",
      " |-- IN_MEDIDOR_GLICOSE: string (nullable = true)\n",
      " |-- IN_MAQUINA_BRAILE: string (nullable = true)\n",
      " |-- IN_SOROBAN: string (nullable = true)\n",
      " |-- IN_MARCA_PASSO: string (nullable = true)\n",
      " |-- IN_SONDA: string (nullable = true)\n",
      " |-- IN_MEDICAMENTOS: string (nullable = true)\n",
      " |-- IN_SALA_INDIVIDUAL: string (nullable = true)\n",
      " |-- IN_SALA_ESPECIAL: string (nullable = true)\n",
      " |-- IN_SALA_ACOMPANHANTE: string (nullable = true)\n",
      " |-- IN_MOBILIARIO_ESPECIFICO: string (nullable = true)\n",
      " |-- IN_MATERIAL_ESPECIFICO: string (nullable = true)\n",
      " |-- IN_NOME_SOCIAL: string (nullable = true)\n",
      " |-- CO_MUNICIPIO_PROVA: string (nullable = true)\n",
      " |-- NO_MUNICIPIO_PROVA: string (nullable = true)\n",
      " |-- CO_UF_PROVA: string (nullable = true)\n",
      " |-- SG_UF_PROVA: string (nullable = true)\n",
      " |-- TP_PRESENCA_CN: string (nullable = true)\n",
      " |-- TP_PRESENCA_CH: string (nullable = true)\n",
      " |-- TP_PRESENCA_LC: string (nullable = true)\n",
      " |-- TP_PRESENCA_MT: string (nullable = true)\n",
      " |-- CO_PROVA_CN: string (nullable = true)\n",
      " |-- CO_PROVA_CH: string (nullable = true)\n",
      " |-- CO_PROVA_LC: string (nullable = true)\n",
      " |-- CO_PROVA_MT: string (nullable = true)\n",
      " |-- NU_NOTA_CN: string (nullable = true)\n",
      " |-- NU_NOTA_CH: string (nullable = true)\n",
      " |-- NU_NOTA_LC: string (nullable = true)\n",
      " |-- NU_NOTA_MT: string (nullable = true)\n",
      " |-- TX_RESPOSTAS_CN: string (nullable = true)\n",
      " |-- TX_RESPOSTAS_CH: string (nullable = true)\n",
      " |-- TX_RESPOSTAS_LC: string (nullable = true)\n",
      " |-- TX_RESPOSTAS_MT: string (nullable = true)\n",
      " |-- TP_LINGUA: string (nullable = true)\n",
      " |-- TX_GABARITO_CN: string (nullable = true)\n",
      " |-- TX_GABARITO_CH: string (nullable = true)\n",
      " |-- TX_GABARITO_LC: string (nullable = true)\n",
      " |-- TX_GABARITO_MT: string (nullable = true)\n",
      " |-- TP_STATUS_REDACAO: string (nullable = true)\n",
      " |-- NU_NOTA_COMP1: string (nullable = true)\n",
      " |-- NU_NOTA_COMP2: string (nullable = true)\n",
      " |-- NU_NOTA_COMP3: string (nullable = true)\n",
      " |-- NU_NOTA_COMP4: string (nullable = true)\n",
      " |-- NU_NOTA_COMP5: string (nullable = true)\n",
      " |-- NU_NOTA_REDACAO: string (nullable = true)\n",
      " |-- Q001: string (nullable = true)\n",
      " |-- Q002: string (nullable = true)\n",
      " |-- Q003: string (nullable = true)\n",
      " |-- Q004: string (nullable = true)\n",
      " |-- Q005: string (nullable = true)\n",
      " |-- Q006: string (nullable = true)\n",
      " |-- Q007: string (nullable = true)\n",
      " |-- Q008: string (nullable = true)\n",
      " |-- Q009: string (nullable = true)\n",
      " |-- Q010: string (nullable = true)\n",
      " |-- Q011: string (nullable = true)\n",
      " |-- Q012: string (nullable = true)\n",
      " |-- Q013: string (nullable = true)\n",
      " |-- Q014: string (nullable = true)\n",
      " |-- Q015: string (nullable = true)\n",
      " |-- Q016: string (nullable = true)\n",
      " |-- Q017: string (nullable = true)\n",
      " |-- Q018: string (nullable = true)\n",
      " |-- Q019: string (nullable = true)\n",
      " |-- Q020: string (nullable = true)\n",
      " |-- Q021: string (nullable = true)\n",
      " |-- Q022: string (nullable = true)\n",
      " |-- Q023: string (nullable = true)\n",
      " |-- Q024: string (nullable = true)\n",
      " |-- Q025: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e813df-1450-4b8e-94a7-0301e32a9682",
   "metadata": {},
   "source": [
    "### Conferindo os dados que foram importados\n",
    "- Quando importamos os dados é importante termos certeza do tipo de dado que consta no schema que foi importado\n",
    "- O schema é a estrutura do dataframe, ou seja, o tipo de dado que a coluna tem, podendo ser definido na hora da importação, utilizando o metodo [StructType](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html) que recebe uma lista de [StructField](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.StructField.html). Dessa forma conseguimos ter certeza do tipo de dado que estamos lidando após a importanção. Isso é importante uma vez que queremos jogar o dado para um banco relacional ou para um DW já estruturado, então temos que ter um controle do dado que estamos lidando. Ou um campo de timestamp que queremos lidar apenas com a data e não com todo o conteudo.\n",
    "- Com isso, após a importação, se quisermeos verificar o que foi importado podemos usar o metodo ```printSchema()``` que retorna a estrutura do dataframe importado.\n",
    "- Vale notar no exemplo da importação do csv, sem o separador e o header a diferença no schema que é retornado.\n",
    "\n",
    "### Tipos de APIS\n",
    "\n",
    "- Dentro do Spark temos atualmente 3 formas de dispor os dados\n",
    "    - RDD - Resilient Distributed Datasets\n",
    "         - Dataset distritubuido resiliente\n",
    "           - Foi o primeiro modelo a ser utilizado no spark, sendo o mais antigo e a estrutura fundamental no funcionamento do spark.\n",
    "           - A Criação dos RDDs pode ser feita através de paralelização de uma lista por exemplo, conseguimos fazer a transformação de um RDD em um dataframe e da mesma forma de um dataframe em um RDD.\n",
    "           - O uso de RDD é recomendado quando queremos fazer transformação em um nivel mais baixo no conjunto de dados que temos.\n",
    "           - O RDD é muito utilizado quando operamos com chave valor, para um conjunto de dados com duas colunas por exemplo.\n",
    "           - Talvez o principalmente entrave do RDD é a necessidade de ser definido o tipo de dado que estamos lidando\n",
    "                - Caso o RDD seja formado de um arquivo importado e não de um dataframe, o spark precisa ser informado exatamente cada formato de dado que esta sendo passado para poder operar de acordo.\n",
    "           - Com um RDD podemos fazer as transformações em conjunto com outros RDDS, ou seja, operações de joins e union\n",
    "           - Podemos operar com o RDD sozinho, fazendo operações internas nele, como operações matematicas ou de filtros por exemplos\n",
    "           - O resto das operações possiveis podemos ver na [documentação](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)\n",
    " - Dataframe\n",
    "      - O Dataframe resolve alguns dos entraves que o RDD cria no processo de engenharia\n",
    "      - Podemos acompanhar o processo de tranformação dos dados na medida que ele vai ocorrendo.\n",
    "      - Conseguimos importar com facilidade arquivos CSV, Parquet, AVRO, HDFS ou HIVE, além de JDBC\n",
    "      - Utilizando o sparkSQL conseguimos executar operações SQL em cima do dataframe\n",
    " - Datasets \n",
    "      - Basicamente junta os dois elementos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
