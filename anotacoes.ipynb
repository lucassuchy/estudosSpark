{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c640e22d-1c6a-47f4-858a-145ff5d4a5d2",
   "metadata": {},
   "source": [
    "Notebook de base para armazenar as minhas anotações dos cursos.\n",
    "\n",
    "Tomarei como base os microdados do enem de 2019. Disponibilizados no [link](https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/enem) . Consultado em 07/01/2021\n",
    "\n",
    "Por Motivos de Volume de dados e possivel problemas de privacidade eu adicionei a pasta de dados, onde eu vou deixar os dados que vou utilizar nos estudos, no .gitignore desse repositorio. Caso seja do interesse reproduzir o conteudo do que esta ocorrendo aqui, recomendo criar a pasta da mesma forma. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff728bdf-578b-42d1-99eb-ca67b4854f96",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "- A instalação do pyspark varia, dependendo do sistema operacional. Como sempre, recomendo que seja utilizado o linux para a instalação. uma VM já resolve. No meu caso estou rodando o Archlinux\n",
    "\n",
    "- Para a instalação do Spark, recomendo que seja seguido o tutorial que foi disponibilizado pelo pessoal da sundog Education, para [linux](https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm), para windows e mac podem ser acompanhado conforme consta no [site](https://sundog-education.com/spark-python/) deles.\n",
    "\n",
    "- Importante lembrar que, o uso do spark não é recomendado para situações de maquina local, uma vez que, caso seja viavel rodar em um nó unico podemos perfeitamente utilizar o pandas. Eu particularmente gosto de usar o pyspark em uma configuração local para testes menores e preparo para uso em um cluster de produção ou infraestrutura similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760916d-bcbb-4561-8328-e4c6dee45748",
   "metadata": {},
   "source": [
    "### importando o pySpark\n",
    "\n",
    "- Após a instalação, para importar a biblioteca base do pyspark funciona de forma muito simples, com o comando import\n",
    "\n",
    "``` import pyspark ``` \n",
    "\n",
    "- Eu não recomendo que seja utilizado o findmyspark, já que em um ambiente de produção, um AWS EMR por exemplo, não existe a necessidade de ser importado dessa forma, o que pode levar a retrabalhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722d94ad-6f79-4604-b1e8-74412fe5eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc55b4e-0669-41b9-b2b3-45bd131018aa",
   "metadata": {},
   "source": [
    "### Criando a sessão\n",
    "- Feito a importação, para executarmos qualquer coisa no spark, precisamos que seja criada uma sessão.\n",
    "    - A sessão no spark pertence a biblioteca SQL, então para criar precissamos chamar a biblioteca SQL e então o SparkSession.\n",
    "    - Do SparkSession, precisamos do metodo builder, que funciona para montar a sessão.\n",
    "    - Após, precisamos do metodo getOrCreate que cria a sessão de fato.\n",
    "    - A Sessão que criarmos podemos armazenar em uma variavel do python, para podemos executar mais etapas do nosso código.\n",
    "    - Assim o comando para criarmos a sessão fica:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21328fbc-cb33-4266-b8dc-86c006b58eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessao = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8780a-cae8-45a0-aee7-859a80043bb5",
   "metadata": {},
   "source": [
    "- Com a Sessão criada, agora podemos executar comandos no spark.\n",
    "\n",
    "### Importando dados\n",
    "- Como vamos usar o spark para processos de ETL, vamos precisar fazer a importação de dados. O spark de forma nativa é capaz de ler os principais tipos de dados que são capazes de se comportar bem com volume altos de dados, então CSV, Parquet, avro, json. O uso do jdbc é bem comum para consulta em bancos de dados relacionais. \n",
    "- Para importar um csv que esta armazenado de forma local, podemos usar o metodo ```read```  na sessão do spark que criamos previamente. Após o metodo read precisamos informar qual tipo de dado que vamos ler, nesse caso vamos ler um arquivo csv, então usamos a função ```csv``` e apontamos o caminho do arquivo que vamos ler, podendo ele estar dentro de um hdfs, um bucket ou local como é o nosso caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df7076a-85bb-4e27-8bcb-b36ee5251839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessao.read.csv('dados/MICRODADOS_ENEM_2019.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
